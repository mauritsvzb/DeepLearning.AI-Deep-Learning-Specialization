# ğŸ“„ Summary
This course will teach you how to build models for natural language, audio, and other sequence data. Thanks to deep learning, sequence algorithms are working far better than just two years ago, and this is enabling numerous exciting applications in speech recognition, music synthesis, chatbots, machine translation, natural language understanding, and many others.

You will:
* Understand how to build and train Recurrent Neural Networks (RNNs), and commonly-used variants such as GRUs and LSTMs
* Be able to apply sequence models to natural language problems, including text synthesis
* Be able to apply sequence models to audio applications, including speech recognition and music synthesis
* Build and train a Transformer model
* This is the fifth and final course of the Deep Learning Specialization

# ğŸ“‚ Projects
* Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step
* Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling
* Week 1 - PA 3 - Jazz improvisation with LSTM
* Week 2 - PA 1 - Word Vector Representation and Debiasing
* Week 2 - PA 2 - Emojify!
* Week 3 - PA 1 - Neural Machine Translation with Attention
* Week 3 - PA 2 - Trigger Word Detection
* Week 4 - PA 1 - Transformer Network
* Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition
* Week 3 - PA 2 - Transformer Network Application: Question Answering

# ğŸ“„ References


# ğŸ† Certificates
To verify the certificates, click the images to follow the links:
